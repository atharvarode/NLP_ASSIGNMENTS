{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":799923,"sourceType":"datasetVersion","datasetId":374}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport transformers\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import model_selection, metrics","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-09-01T15:32:08.045718Z","iopub.execute_input":"2024-09-01T15:32:08.046160Z","iopub.status.idle":"2024-09-01T15:32:08.051904Z","shell.execute_reply.started":"2024-09-01T15:32:08.046119Z","shell.execute_reply":"2024-09-01T15:32:08.050728Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the¬†current¬†session","metadata":{"execution":{"iopub.status.busy":"2024-09-01T15:32:08.078770Z","iopub.execute_input":"2024-09-01T15:32:08.079707Z","iopub.status.idle":"2024-09-01T15:32:08.091510Z","shell.execute_reply.started":"2024-09-01T15:32:08.079641Z","shell.execute_reply":"2024-09-01T15:32:08.090378Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/stanford-question-answering-dataset/train-v1.1.json\n/kaggle/input/stanford-question-answering-dataset/dev-v1.1.json\n","output_type":"stream"}]},{"cell_type":"code","source":"import json\ndata = json.load(open('/kaggle/input/stanford-question-answering-dataset/train-v1.1.json'))","metadata":{"execution":{"iopub.status.busy":"2024-09-01T15:32:08.093788Z","iopub.execute_input":"2024-09-01T15:32:08.094222Z","iopub.status.idle":"2024-09-01T15:32:08.992812Z","shell.execute_reply.started":"2024-09-01T15:32:08.094173Z","shell.execute_reply":"2024-09-01T15:32:08.991618Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"listt = []\nfor i in data['data']:\n    d = {}\n    for para in i['paragraphs']:\n            context = para['context']\n            for qa in para['qas']:\n                d['answer'] = qa['answers'][0]['answer_start']\n                d['text'] = qa['answers'][0]['text']\n                d['question'] = qa['question']\n                d['context'] = context\n    listt.append(d)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-01T15:32:08.995067Z","iopub.execute_input":"2024-09-01T15:32:08.995537Z","iopub.status.idle":"2024-09-01T15:32:09.087265Z","shell.execute_reply.started":"2024-09-01T15:32:08.995485Z","shell.execute_reply":"2024-09-01T15:32:09.086026Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(listt)","metadata":{"execution":{"iopub.status.busy":"2024-09-01T15:32:09.088668Z","iopub.execute_input":"2024-09-01T15:32:09.089103Z","iopub.status.idle":"2024-09-01T15:32:09.098200Z","shell.execute_reply.started":"2024-09-01T15:32:09.089038Z","shell.execute_reply":"2024-09-01T15:32:09.096978Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-09-01T15:32:09.101603Z","iopub.execute_input":"2024-09-01T15:32:09.102048Z","iopub.status.idle":"2024-09-01T15:32:09.130018Z","shell.execute_reply.started":"2024-09-01T15:32:09.101995Z","shell.execute_reply":"2024-09-01T15:32:09.128833Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"     answer                                               text  \\\n0      1232                                      Jim Wetherbee   \n1       533                     Salma Hayek and Frida Giannini   \n2       926       Great Falls, Lewistown, Cut Bank and Glasgow   \n3       995                        humanizing a devalued group   \n4       763                                     Allan Coukell,   \n..      ...                                                ...   \n437     456  establish a restaurant guest's identity and fo...   \n438      57                             Ghazals and folk songs   \n439     955                        being bitten during a fight   \n440     404                                           poaching   \n441       0                        Kathmandu Metropolitan City   \n\n                                              question  \\\n0    Which notable astronaut is known to have atten...   \n1    Who did Beyonc√© work with in 2013 on the Chime...   \n2               Where were air bases built in Montana?   \n3    What is one preventive effort in circumventing...   \n4      Who is a director at the Pew Charitable Trusts?   \n..                                                 ...   \n437  How could police help the owner when a restaur...   \n438  What kind of music does Roshen Ara Begum perform?   \n439              How did tyrannosaurs become infected?   \n440  What else is partly to blame for the declining...   \n441                      What is KMC an initialism of?   \n\n                                               context  \n0    Notre Dame alumni work in various fields. Alum...  \n1    In December, Beyonc√© along with a variety of o...  \n2    When the U.S. entered World War II on December...  \n3    Other authors have focused on the structural c...  \n4    Possible improvements include clarification of...  \n..                                                 ...  \n437  In contrast, the police are entitled to protec...  \n438  For the popular taste however, light music, pa...  \n439  Evidence of infection in fossil remains is a s...  \n440  In contrast, Botswana has recently been forced...  \n441  Kathmandu Metropolitan City (KMC), in order to...  \n\n[442 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>answer</th>\n      <th>text</th>\n      <th>question</th>\n      <th>context</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1232</td>\n      <td>Jim Wetherbee</td>\n      <td>Which notable astronaut is known to have atten...</td>\n      <td>Notre Dame alumni work in various fields. Alum...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>533</td>\n      <td>Salma Hayek and Frida Giannini</td>\n      <td>Who did Beyonc√© work with in 2013 on the Chime...</td>\n      <td>In December, Beyonc√© along with a variety of o...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>926</td>\n      <td>Great Falls, Lewistown, Cut Bank and Glasgow</td>\n      <td>Where were air bases built in Montana?</td>\n      <td>When the U.S. entered World War II on December...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>995</td>\n      <td>humanizing a devalued group</td>\n      <td>What is one preventive effort in circumventing...</td>\n      <td>Other authors have focused on the structural c...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>763</td>\n      <td>Allan Coukell,</td>\n      <td>Who is a director at the Pew Charitable Trusts?</td>\n      <td>Possible improvements include clarification of...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>437</th>\n      <td>456</td>\n      <td>establish a restaurant guest's identity and fo...</td>\n      <td>How could police help the owner when a restaur...</td>\n      <td>In contrast, the police are entitled to protec...</td>\n    </tr>\n    <tr>\n      <th>438</th>\n      <td>57</td>\n      <td>Ghazals and folk songs</td>\n      <td>What kind of music does Roshen Ara Begum perform?</td>\n      <td>For the popular taste however, light music, pa...</td>\n    </tr>\n    <tr>\n      <th>439</th>\n      <td>955</td>\n      <td>being bitten during a fight</td>\n      <td>How did tyrannosaurs become infected?</td>\n      <td>Evidence of infection in fossil remains is a s...</td>\n    </tr>\n    <tr>\n      <th>440</th>\n      <td>404</td>\n      <td>poaching</td>\n      <td>What else is partly to blame for the declining...</td>\n      <td>In contrast, Botswana has recently been forced...</td>\n    </tr>\n    <tr>\n      <th>441</th>\n      <td>0</td>\n      <td>Kathmandu Metropolitan City</td>\n      <td>What is KMC an initialism of?</td>\n      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n    </tr>\n  </tbody>\n</table>\n<p>442 rows √ó 4 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"config = {\n    \"max_length\": 512,\n    \"model_path\": \"microsoft/xtremedistil-l6-h256-uncased\",\n    \n    \"output_dir\": \"./my-model\",\n    \"train_batch_size\": 64,\n    \"valid_batch_size\": 64,\n    \"learning_rate\": 3e-5,\n    \"epochs\": 300,\n    \n    \"debug\": True,\n}","metadata":{"execution":{"iopub.status.busy":"2024-09-01T15:32:09.131396Z","iopub.execute_input":"2024-09-01T15:32:09.131848Z","iopub.status.idle":"2024-09-01T15:32:09.138083Z","shell.execute_reply.started":"2024-09-01T15:32:09.131807Z","shell.execute_reply":"2024-09-01T15:32:09.136993Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"tokenizer = transformers.AutoTokenizer.from_pretrained(config[\"model_path\"])\nclass TextDataset:\n    \n    def __init__(self, data):\n        self.data = data\n        \n    def __len__(self):\n        return self.data.shape[0]\n    \n    def preprocess_function(self,question, context, answer_start_char, answer_end_char):\n        inputs = tokenizer(\n            question,\n            context,\n            max_length=config[\"max_length\"],\n            truncation=\"only_second\",\n            return_offsets_mapping=True,\n            padding=\"max_length\",\n        )\n\n        offset = inputs.pop(\"offset_mapping\") \n        sequence_ids = inputs.sequence_ids()\n\n        context_start, context_end = -1, -1\n\n\n        # Add logic to find the token indices for context start and context end using `sequence_ids``.\n        for index in range(len(sequence_ids)):\n            i = sequence_ids[index]\n            if context_start == -1:\n                if i == 1:\n                    context_start = index\n            else:\n                if i != 1:\n                    context_end = index          \n\n        context_offsets = offset[context_start: context_end]\n\n        # Create a mapping of character index to token index.\n        character_pos_to_token_pos = {}\n        for token_pos, (char_start, char_end) in enumerate(context_offsets):\n            token_pos1 = context_start + token_pos\n            for i in range(char_start, char_end+1):\n                character_pos_to_token_pos[i] = token_pos1\n\n        start_pos = character_pos_to_token_pos.get(answer_start_char, 0)\n        end_pos = character_pos_to_token_pos.get(\n            answer_end_char - 1, \n            0 if start_pos == 0 else config['max_length'] - 1\n        )\n\n        inputs[\"start_positions\"] = start_pos\n        inputs[\"end_positions\"] = end_pos\n\n        return inputs\n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        \n        question = row['question']\n        context = row['context']\n        answer_start = row['answer']\n        answer_end = answer_start + len(row['text'])\n        \n        return self.preprocess_function(question, context, answer_start,answer_end)","metadata":{"execution":{"iopub.status.busy":"2024-09-01T15:32:09.139690Z","iopub.execute_input":"2024-09-01T15:32:09.140457Z","iopub.status.idle":"2024-09-01T15:32:09.896639Z","shell.execute_reply.started":"2024-09-01T15:32:09.140310Z","shell.execute_reply":"2024-09-01T15:32:09.895324Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/525 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c10260d55a1643f989a99214d8ae880b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9808d7dd74b4054bdbad21076cb73a8"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"dff = TextDataset(df)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-01T15:32:09.898384Z","iopub.execute_input":"2024-09-01T15:32:09.898738Z","iopub.status.idle":"2024-09-01T15:32:09.903737Z","shell.execute_reply.started":"2024-09-01T15:32:09.898702Z","shell.execute_reply":"2024-09-01T15:32:09.902210Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def preprocess_function(question, context, answer_start_char, answer_end_char):\n    inputs = tokenizer(\n        question,\n        context,\n        max_length=config[\"max_length\"],\n        truncation=\"only_second\",\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n    \n    offset = inputs.pop(\"offset_mapping\") \n    sequence_ids = inputs.sequence_ids()\n    \n    context_start, context_end = -1, -1\n    \n\n    # Add logic to find the token indices for context start and context end using `sequence_ids``.\n    for index in range(len(sequence_ids)):\n        i = sequence_ids[index]\n        print(i)\n        if context_start == -1:\n            if i == 1:\n                context_start = index\n        else:\n            if i != 1:\n                context_end = index          \n    \n    context_offsets = offset[context_start: context_end]\n    \n    # Create a mapping of character index to token index.\n    character_pos_to_token_pos = {}\n    for token_pos, (char_start, char_end) in enumerate(context_offsets):\n        token_pos1 = context_start + token_pos\n        for i in range(char_start, char_end+1):\n            character_pos_to_token_pos[i] = token_pos1\n            \n    start_pos = charcter_pos_to_token_pos.get(answer_start_char, 0)\n    end_pos = charcter_pos_to_token_pos.get(\n        answer_end_char - 1, \n        0 if start_pos == 0 else config['max_length'] - 1\n    )\n        \n    inputs[\"start_positions\"] = start_pos\n    inputs[\"end_positions\"] = end_pos\n        \n    return inputs","metadata":{"execution":{"iopub.status.busy":"2024-09-01T15:32:09.905349Z","iopub.execute_input":"2024-09-01T15:32:09.905690Z","iopub.status.idle":"2024-09-01T15:32:09.917712Z","shell.execute_reply.started":"2024-09-01T15:32:09.905645Z","shell.execute_reply":"2024-09-01T15:32:09.916599Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train, valid = model_selection.train_test_split(\n    df,\n    test_size=0.2,\n    shuffle=True\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-01T15:32:09.919030Z","iopub.execute_input":"2024-09-01T15:32:09.920753Z","iopub.status.idle":"2024-09-01T15:32:09.933169Z","shell.execute_reply.started":"2024-09-01T15:32:09.920703Z","shell.execute_reply":"2024-09-01T15:32:09.932112Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train_ds = TextDataset(train)\nvalid_ds = TextDataset(valid)","metadata":{"execution":{"iopub.status.busy":"2024-09-01T15:32:09.937799Z","iopub.execute_input":"2024-09-01T15:32:09.938218Z","iopub.status.idle":"2024-09-01T15:32:09.943381Z","shell.execute_reply.started":"2024-09-01T15:32:09.938178Z","shell.execute_reply":"2024-09-01T15:32:09.942242Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"model = transformers.AutoModelForQuestionAnswering.from_pretrained(config[\"model_path\"])","metadata":{"execution":{"iopub.status.busy":"2024-09-01T15:32:09.944780Z","iopub.execute_input":"2024-09-01T15:32:09.945446Z","iopub.status.idle":"2024-09-01T15:32:11.031700Z","shell.execute_reply.started":"2024-09-01T15:32:09.945397Z","shell.execute_reply":"2024-09-01T15:32:11.030973Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/51.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74ee3f42d59a4f72ab36add3a604250f"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at microsoft/xtremedistil-l6-h256-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"import pickle\nfrom datasets import load_metric\nimport numpy as np\n\ndef compute_metrics(eval_pred):\n    # Load metrics\n    exact_match_metric = load_metric(\"exact_match\")\n    f1_metric = load_metric(\"f1\")\n\n    start_logits, end_logits = eval_pred.predictions\n    examples = eval_pred.label_ids\n    \n    # Extract information from examples\n    example_ids = examples[\"example_id\"]\n    contexts = examples[\"context\"]\n    offset_mappings = examples[\"offset_mapping\"]\n    ground_truth_answers = examples[\"answers\"]  # Assume this contains the true answers\n    \n    exact_match = 0\n    f1 = 0\n    \n    for i in range(len(example_ids)):\n        # Get the predicted start and end positions\n        start_logit = start_logits[i]\n        end_logit = end_logits[i]\n        \n        start_index = np.argmax(start_logit)\n        end_index = np.argmax(end_logit)\n        \n        # Get the offset mapping for this example\n        offsets = offset_mappings[i]\n        \n        # Get the predicted answer span\n        pred_start_char = offsets[start_index][0]\n        pred_end_char = offsets[end_index][1]\n        \n        pred_answer = contexts[i][pred_start_char:pred_end_char]\n        \n        # Get the ground truth answer\n        ground_truth_answer = ground_truth_answers[i]\n        \n        # Compute exact match\n        if pred_answer.strip() == ground_truth_answer.strip():\n            exact_match += 1\n        \n        # Compute F1 score\n        f1 += compute_f1(pred_answer, ground_truth_answer)\n    \n    total = len(example_ids)\n    avg_exact_match = exact_match / total\n    avg_f1 = f1 / total\n    \n    return {\n        \"exact_match\": avg_exact_match,\n        \"f1\": avg_f1\n    }\n\ndef compute_f1(pred_answer, ground_truth_answer):\n    # Function to compute F1 score\n    from sklearn.metrics import f1_score\n    \n    # Tokenize answers for F1 score calculation\n    pred_tokens = pred_answer.split()\n    ground_truth_tokens = ground_truth_answer.split()\n    \n    # Handle cases where there are no tokens\n    if not pred_tokens:\n        return 0 if ground_truth_tokens else 1\n    \n    if not ground_truth_tokens:\n        return 0\n    \n    return f1_score(ground_truth_tokens, pred_tokens, average=\"micro\")\n\n\n\ntraining_args = transformers.TrainingArguments(\n     output_dir=\"./results\",                      # Directory for storing results\n    evaluation_strategy=\"steps\",                 # Evaluate every few steps\n    per_device_train_batch_size=config['train_batch_size'],              # Batch size per device during training\n    per_device_eval_batch_size=config['train_batch_size'],               # Batch size per device during evaluation\n    num_train_epochs=config['epochs'],                          # Total number of training epochs\n    warmup_steps=500,                            # Number of warmup steps for learning rate scheduler\n    save_total_limit=2,\n    logging_dir=None,                            # Disable logging directory\n    logging_strategy=\"no\",\n    report_to=[]# Limit the total amount of checkpoints`\n\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-01T15:32:11.033162Z","iopub.execute_input":"2024-09-01T15:32:11.033511Z","iopub.status.idle":"2024-09-01T15:32:11.692957Z","shell.execute_reply.started":"2024-09-01T15:32:11.033451Z","shell.execute_reply":"2024-09-01T15:32:11.691933Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer = transformers.Trainer(\n    model=model,                                 # The model to be trained\n    args=training_args,                          # The training arguments, defined above\n    train_dataset=train_ds,                 # The training dataset\n    eval_dataset=valid_ds,                   # The evaluation dataset\n    tokenizer=tokenizer,                         # The tokenizer\n    compute_metrics=compute_metrics, \n)","metadata":{"execution":{"iopub.status.busy":"2024-09-01T15:32:11.694376Z","iopub.execute_input":"2024-09-01T15:32:11.695058Z","iopub.status.idle":"2024-09-01T15:32:27.011450Z","shell.execute_reply.started":"2024-09-01T15:32:11.695019Z","shell.execute_reply":"2024-09-01T15:32:27.010432Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-09-01T15:32:27.012898Z","iopub.execute_input":"2024-09-01T15:32:27.013775Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='501' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [501/900 06:47 < 05:25, 1.22 it/s, Epoch 166.67/300]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>\n    <div>\n      \n      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/1 : < :]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/2201463457.py:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n  exact_match_metric = load_metric(\"exact_match\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/1.63k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d071f969daf5409c898cad061a7c8b27"}},"metadata":{}}]},{"cell_type":"code","source":"results = trainer.evaluate()\n\n# Print evaluation results\nprint(results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.save_state()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.save_model()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline\n\n# Load the QnA pipeline with a pre-trained model\nqna_pipeline = pipeline(\"question-answering\", model=\"microsoft/xtremedistil-l6-h256-uncased\")\n\n# Example input data\nquestion = \"What is the capital of France?\"\ncontext = \"France is a country in Europe. The capital of France is Paris.\"\n\n# Perform inference\nresult = qna_pipeline(question=question, context=context)\n\nprint(result)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}